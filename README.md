# Neural Network Activation Functions Comparison
This project compares the performance of neural networks using different activation functions on the Breast Cancer dataset. It evaluates and visualizes the impact of Sigmoid, ReLU, and Leaky ReLU activation functions on model accuracy, loss, and gradient norms.

Key Steps:

Data Preparation: Load and preprocess the Breast Cancer dataset.
Model Training: Build and train neural networks with various activation functions.
Evaluation: Assess model accuracy, loss, and gradient norms.
Visualization: Plot accuracy, loss, and gradient norms to compare activation functions.
Libraries Used:

- TensorFlow
- NumPy
- Pandas
- Matplotlib
- Scikit-learn
